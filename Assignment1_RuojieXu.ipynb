{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Assignment A1\n",
    "\n",
    "This assessment is marked out of 50 and comprises 50% of the final course mark.\n",
    "\n",
    "Due by 23:59 on Monday November 18, to be submitted via email to k.zygalakis@ed.ac.uk.\n",
    "\n",
    "### Academic misconduct\n",
    "\n",
    "The assessment is primarily summative in nature. You are expected to be aware of and abide by University policies on academic misconduct.\n",
    "\n",
    "- [School of Mathematics academic misconduct advice and policies](https://teaching.maths.ed.ac.uk/main/undergraduate/studies/assessment/academic-misconduct)\n",
    "- [Academic Services academic misconduct information](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct)\n",
    "\n",
    "**This is an individual assignment - do not copy the work of another student.**\n",
    "\n",
    "If you use any resources (e.g. textbooks or websites) then include appropriate references in your solutions. Course materials do not need to be referenced, but you should clearly state which results you are using.\n",
    "\n",
    "\n",
    "### Code commentary\n",
    "\n",
    "Your code should be extensively commented, with the functionality of each line of code explained with a comment. This is to test your understanding of the code you have written. Up to half of the marks associated with the coding part of a question may be deducted for a missing, incomplete, or inaccurate code commentary.\n",
    "\n",
    "Your comments should explain what the code does, as well as why it does it.\n",
    "\n",
    "The following provides an example of the expected level of commenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    \"\"\"\n",
    "    Return whether an input positive integer is prime.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n == 1:        # If n is 1 ...\n",
    "        return False  # ... then n is not prime\n",
    "    \n",
    "    for i in range(2, n):  # Test integers i from 2 to n - 1 inclusive\n",
    "        if n % i == 0:     # If n is divisible by i ...\n",
    "            return False   # ... then n is not prime\n",
    "    # If n is not divisible by any integers from 2 to n - 1 inclusive then n is\n",
    "    # prime\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output \n",
    "\n",
    "Your code must generate and display all relevant output when run. Rerun your code cells after editing your code, to make sure that the output is updated.\n",
    "\n",
    "### Markdown cells\n",
    "\n",
    "You can enter your answers to theoretical questions in the Markdown cells provided in this notebook. To start editing the cell, press shift+enter or double click on it. You can use basic Latex. To render the cell, press shift+enter or run.\n",
    "\n",
    "Alternatively, you can submit a pdf of your hand-written and scanned in answers to the theoretical questions on Learn, alongside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Polynomial Regression\n",
    "\n",
    "Consider the least squares regression problem introduced in lectures, where we wish to reconstruct an unknown function from noisy point values. Suppose we are given a set of points $\\{x_i, y_i\\}_{i=1}^m$, and we want to find the degree $n-1$ polynomial that best fits these points.  In other words, we want to learn the unknown coefficients $\\{u_j\\}_{j=1}^n$ in the polynomial\n",
    "$$\n",
    "f(x; u) = \\sum_{j=1}^{n} {u_j} x^{j-1}, \n",
    "$$\n",
    "such that the residual $\\sum_{i=1}^m (y_i - f(x_i;u))^2$ is minimised.\n",
    "\n",
    "\n",
    "### 1.1\n",
    "\n",
    "Show that the regression problem above can be formulated as the linear inverse problem\n",
    "\n",
    "$$\n",
    "u^* = \\text{argmin} \\frac{1}{2} \\|A u - y\\|_2^2,\n",
    "$$\n",
    "\n",
    "where $y = \\{y_i\\}_{i=1}^m \\in \\mathbb R^m$ and $A \\in \\mathbb R^{m \\times n}$ is the Vandermonde matrix with entries $a_{ij} = x_i^{j-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $(\\mathbf{A}u-y)_i=\\sum_{j=1}^n a_{ij}u_j-y_i$,\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\n",
    "    &=\\sum_{i=1}^m \\Big( \\sum_{j=1}^n a_{ij}u_j-y_i\\Big)^2\\\\\n",
    "    &=\\sum_{i=1}^m \\Big( \\sum_{j=1}^n x_i^{j-1}u_j-y_i\\Big)^2\\\\\n",
    "    &=\\sum_{i=1}^m \\Big( f(x_i;u)-y_i\\Big)^2\\\\\n",
    "    &=\\sum_{i=1}^m \\Big(y_i-f(x_i;u)\\Big)^2.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "Hence, \n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    u^{*}&=\\arg\\min\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\\\\n",
    "    &=\\arg\\min\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\\\\n",
    "    &=\\arg\\min \\sum_{i=1}^m \\Big(y_i-f(x_i;u)\\Big)^2.\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "\n",
    "Show that the gradient of $\\frac{1}{2} \\|A u - y\\|_2^2$ with respect to $u$ is \n",
    "\n",
    "$$A^\\mathrm T A u - A^\\mathrm T y.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2$ is the objective function.\n",
    "\n",
    "Since $\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2=\\frac{1}{2}\\sum_{i=1}^m \\Big( \\sum_{j=1}^n a_{ij}u_j-y_i\\Big)^2$, for $k=1,2,\\ldots,n$, we have\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\Big(\\frac{\\partial}{\\partial u}\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\Big)_k\n",
    "    &=\\frac{\\partial}{\\partial u_k}\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\\\\n",
    "    &=\\frac{1}{2}\\frac{\\partial}{\\partial u_k}\\sum_{i=1}^m \\Big( \\sum_{j=1}^n a_{ij}u_j-y_i\\Big)^2\\\\\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^m \\frac{\\partial}{\\partial u_k}\\Big( \\sum_{j=1}^n a_{ij}u_j-y_i\\Big)^2\\\\\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^m 2\\Big(\\sum_{j=1}^n a_{ij}u_j-y_i\\Big)a_{ik}\\\\\n",
    "    &=\\sum_{i=1}^m a_{ik}\\Big( \\sum_{j=1}^n a_{ij}u_j-y_i\\Big)\\\\\n",
    "    &=\\sum_{i=1}^m a_{ik} (\\mathbf{A}u-y)_i\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "where $(a_{1k},\\ldots,a_{mk})$ is the $k^{th}$ column vector of $\\mathbf{A}$ and the $k^{th}$ row vector of $\\mathbf{A^T}$, $(\\mathbf{A}u-y)$ is a $m\\times 1$ column vector. \n",
    "\n",
    "Let $(\\mathbf{A^T})^{row}_k$ denote the $k^{th}$ row vector of $\\mathbf{A^T}$, then\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\Big(\\frac{\\partial}{\\partial u}\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\Big)_k\n",
    "    &= (\\mathbf{A^T})^{row}_k (\\mathbf{A}u-y).\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "Hence, \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u}\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2=\\mathbf{A^T}(\\mathbf{A}u-y)= \\mathbf{A^T}\\mathbf{A}u-\\mathbf{A^T}y.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 \n",
    "\n",
    "In the code cell below, implement the method of gradient descent to compute a minimiser $u^*$. You should choose a suitable step size $h$.\n",
    "\n",
    "Test your method on the simple, noise-free example given at the bottom of the code cell, to which the answer is $u = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimiser from the method of gradient descent is [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add code here\n",
    "def GDminimiser(y,A,h,u_0,num):\n",
    "    #h is step size, u_0 is initial point, num is the number of iterations \n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from gradiant descent method\n",
    "    \"\"\"\n",
    "    u=u_0  #starting point\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run num interations\n",
    "        #in each iteration\n",
    "        #claculate the gradient of the objective function with respect to u\n",
    "        delta_u=A.T.dot(A).dot(u)-A.T.dot(y) \n",
    "        #update the value of u via the gradiant method\n",
    "        u=u-delta_u*h \n",
    "        \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "# Testing the function on a small example\n",
    "x = np.array([-0.5, 0, 0.5])\n",
    "y = np.array([0., 1., 2.])\n",
    "A = np.array([[1., -0.5],[1., 0.],[1., 0.5]])\n",
    "\n",
    "u_0=np.array([0,0]) #set the initial value of u as (0,0)\n",
    "#use a constant step size of 1/3\n",
    "print(\"The minimiser from the method of gradient descent is\",GDminimiser(y,A,1/3,u_0,1000) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "\n",
    "In the code cell below, implement the Nestorov method both for convex and strongly convex objectives and  compute a minimiser $u^*$. \n",
    "\n",
    "Test your method on the simple, noise-free example given at the bottom of the code cell, to which the answer is $u = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimiser from Nestorov method for convex objectives is [1. 2.]\n",
      "The minimiser from Nestorov method for strongly convex objectives is [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add code here\n",
    "#This is an optimisaion problem for least-squares estimator, so L equals to the largest eigenvalue \n",
    "#of A.T.dot(A), mu equals to the smallest eigenvalue of A.T.dot(A).\n",
    "L=3     #L is Lipschitz constant of the first derivative of the objective function\n",
    "mu=0.5  #the objective function is mu-strongly convex\n",
    "\n",
    "def convex_min(y,A,u_0,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    k=1    #index for iteration\n",
    "    t_minus1=1  #initial value of a sequence values of t\n",
    "    t=t_minus1  #current value of t\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        t_plus=((1+4*(t**2))**(0.5)+1)/2   #calculate the new value for t\n",
    "        gamma=(t-1)/t_plus                 #calculate the value of gamma\n",
    "        t=t_plus                           #update the value of t\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=A.T.dot(A).dot(v)-A.T.dot(y) \n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "def Sconvex_min(y,A,u_0,L,mu,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for strongly convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    gamma=(L**(0.5)-mu**(0.5))/(L**(0.5)+mu**(0.5)) #calculate the constant value of gamma\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=A.T.dot(A).dot(v)-A.T.dot(y) \n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "        \n",
    "# Testing the function on a small example\n",
    "x = np.array([-0.5, 0, 0.5])\n",
    "y = np.array([0., 1., 2.])\n",
    "A = np.array([[1., -0.5],[1., 0.],[1., 0.5]])\n",
    "\n",
    "u_0=np.array([0,0]) #set the initial value of u as (0,0)\n",
    "print(\"The minimiser from Nestorov method for convex objectives is\",convex_min(y,A,u_0,1000))\n",
    "print(\"The minimiser from Nestorov method for strongly convex objectives is\",Sconvex_min(y,A,u_0,3,0.5,1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "\n",
    "When $m \\geq n$ (i.e. we are dealing with an overdetermined least squares problem) and $rank(A) = n$ (i.e. the matrix $A$ has full column rank), the minimisation problem in Question 1.1 is strongly convex, and hence has a unique minimiser $u^*$.\n",
    "\n",
    "Compare the performance of gradient descent and Nestorov in computing $u^*$, for the data $\\{x_i, y_i\\}_{i=1}^m$ loaded in the code cell below. You may want to compare the computed solutions to the solution obtained with *np.linalg.lstsq(A,y)[0]*.\n",
    "\n",
    "Include any numerical tests that you run in the code cell below. Any output you use in your discussion should be displayed and easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimiser from the method of gradient descent after 1000 iterations is [-0.06549014 -0.19228465  0.26881368  0.89762087  0.38048177  0.60825961\n",
      " -0.15680148 -1.95149743 -0.82514545  0.        ]\n",
      "The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is [ -0.08024633   3.93926188   2.55264311 -34.69686622 -17.90092821\n",
      "  77.11144624  38.71269974 -48.71937798 -24.76092292   0.        ]\n",
      "Solution obtained with np.linalg.lstsq(A,y)[0] [ -0.10100278   2.39670315   2.57656176 -22.33974528 -15.80921523\n",
      "  51.75181661  32.44369722 -33.71134564 -20.3108598    0.        ]\n",
      "The residual of the method of gradient descent after 1000 iterations is 58.396071289072\n",
      "The residual of Nestorov method for strongly convex objectives after 1000 iterations is 51.46112375714068\n",
      "The residual of using solution obtained with np.linalg.lstsq(A,y)[0] 49.17687495533957\n",
      "The minimiser from the method of gradient descent after 10000 iterations is [-0.02108256  0.02347249 -0.44548385 -1.8751599   1.63865856  7.75238016\n",
      "  0.70036619 -6.81164804 -2.47208183  0.        ]\n",
      "The minimiser from Nestorov method for strongly convex objectives after 10000 iterations is [-2.61291772e-02  3.61913489e+00  6.41888670e-02 -3.26976976e+01\n",
      " -2.81891993e+00  7.36408980e+01  1.11760188e+01 -4.68667215e+01\n",
      " -9.52424018e+00  0.00000000e+00]\n",
      "Solution obtained with np.linalg.lstsq(A,y)[0] [ -0.10100278   2.39670315   2.57656176 -22.33974528 -15.80921523\n",
      "  51.75181661  32.44369722 -33.71134564 -20.3108598    0.        ]\n",
      "The residual of the method of gradient descent after 10000 iterations is 55.89756381896655\n",
      "The residual of Nestorov method for strongly convex objectives after 10000 iterations is 51.26152077962673\n",
      "The residual of using solution obtained with np.linalg.lstsq(A,y)[0] 49.17687495533957\n",
      "The minimiser from the method of gradient descent after 100000 iterations is [-2.23241420e-02  1.86117765e+00 -1.35093211e-01 -1.76428429e+01\n",
      " -1.19035981e+00  4.14877703e+01  7.30281219e+00 -2.73372383e+01\n",
      " -6.85228829e+00  0.00000000e+00]\n",
      "The minimiser from Nestorov method for strongly convex objectives after 100000 iterations is [ -0.19071046   3.08045639   5.67629753 -28.32216765 -32.55849454\n",
      "  64.79523609  61.30939994 -41.79394304 -35.79425038   0.        ]\n",
      "Solution obtained with np.linalg.lstsq(A,y)[0] [ -0.10100278   2.39670315   2.57656176 -22.33974528 -15.80921523\n",
      "  51.75181661  32.44369722 -33.71134564 -20.3108598    0.        ]\n",
      "The residual of the method of gradient descent after 100000 iterations is 49.85273563502068\n",
      "The residual of Nestorov method for strongly convex objectives after 100000 iterations is 50.160798050917364\n",
      "The residual of using solution obtained with np.linalg.lstsq(A,y)[0] 49.17687495533957\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data points {x_i} - uniformly spaced points in [-1,1]\n",
    "x = np.load('data_x.npy')\n",
    "\n",
    "# Load corresponding data points {y_i}\n",
    "y = np.load('data_y.npy')\n",
    "\n",
    "\n",
    "# add your code here\n",
    "m=x.size      #m is the number of data points\n",
    "n=10          #n is the order of the polynomial degree\n",
    "A=np.zeros(m*n)     #create a vector with m*n entries \n",
    "for i in range(1,m):     #for entries of the i^th row of matrix A\n",
    "    for j in range(1,n): #for entries of the i^th column of matrix A\n",
    "        #calculate the j^th component of the i^th row of matrix A\n",
    "        A[(i-1)*n+j-1]=x[i-1]**(j-1) \n",
    "A=A.reshape((m,n)) #reshape A to be a m*n matrix\n",
    "eigen=np.linalg.eigvals(A.T.dot(A)) #eigen is the vector of eigen values of A.T.dot(A)\n",
    "L=eigen.max()  #Set the value of L the largest eigen value of A.T.dot(A)\n",
    "mu=eigen.min() #Set the value of mu the smallest eigen value of A.T.dot(A)\n",
    "\n",
    "##Set the number of iterations as 1000. \n",
    "u_0=np.zeros(n) #set the initial value of all elements of u as 0\n",
    "#use a constant step size of 1/L\n",
    "u_1=GDminimiser(y,A,1/L,u_0,1000)\n",
    "print(\"The minimiser from the method of gradient descent after 1000 iterations is\",u_1)\n",
    "\n",
    "u_2=Sconvex_min(y,A,u_0,L,mu,1000)\n",
    "print(\"The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is\",u_2)\n",
    "\n",
    "u_3=np.linalg.lstsq(A,y,rcond=None)[0]\n",
    "print(\"Solution obtained with np.linalg.lstsq(A,y)[0]\",u_3)\n",
    "\n",
    "def resiValue(u_estimate,x,y,m,n):\n",
    "    \"\"\"\n",
    "    Return the residuals of different methods\n",
    "    \"\"\"\n",
    "    res=0\n",
    "    for i in range(1,m): #we will sum the residuals of all m data points\n",
    "        ux=0  #ux is the fitted value of y\n",
    "        for j in range(1,n):\n",
    "            ux=ux+u_estimate[j-1]*(x[i-1]**(j-1))  #calculate the fitted value for each data point\n",
    "        res=res+(y[i-1]-ux)**2  #cumulative residuals for the first i data points\n",
    "    return res #output is the residuals\n",
    "\n",
    "print(\"The residual of the method of gradient descent after 1000 iterations is\",resiValue(u_1,x,y,m,n))\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 1000 iterations is\",resiValue(u_2,x,y,m,n))\n",
    "print(\"The residual of using solution obtained with np.linalg.lstsq(A,y)[0]\",resiValue(u_3,x,y,m,n))\n",
    "\n",
    "##Set the number of iterations as 10000. \n",
    "u_0=np.zeros(n) #set the initial value of all elements of u as 0\n",
    "#use a constant step size of 1/L\n",
    "u_1=GDminimiser(y,A,1/L,u_0,10000)\n",
    "print(\"The minimiser from the method of gradient descent after 10000 iterations is\",u_1)\n",
    "\n",
    "u_2=Sconvex_min(y,A,u_0,L,mu,10000)\n",
    "print(\"The minimiser from Nestorov method for strongly convex objectives after 10000 iterations is\",u_2)\n",
    "\n",
    "u_3=np.linalg.lstsq(A,y,rcond=None)[0]\n",
    "print(\"Solution obtained with np.linalg.lstsq(A,y)[0]\",u_3)\n",
    "\n",
    "print(\"The residual of the method of gradient descent after 10000 iterations is\",resiValue(u_1,x,y,m,n))\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 10000 iterations is\",resiValue(u_2,x,y,m,n))\n",
    "print(\"The residual of using solution obtained with np.linalg.lstsq(A,y)[0]\",resiValue(u_3,x,y,m,n))\n",
    "            \n",
    "######Set the number of iterations as 100000. \n",
    "u_0=np.zeros(n) #set the initial value of all elements of u as 0\n",
    "#use a constant step size of 1/L\n",
    "u_1=GDminimiser(y,A,1/L,u_0,100000)\n",
    "print(\"The minimiser from the method of gradient descent after 100000 iterations is\",u_1)\n",
    "\n",
    "u_2=Sconvex_min(y,A,u_0,L,mu,100000)\n",
    "print(\"The minimiser from Nestorov method for strongly convex objectives after 100000 iterations is\",u_2)\n",
    "\n",
    "u_3=np.linalg.lstsq(A,y,rcond=None)[0]\n",
    "print(\"Solution obtained with np.linalg.lstsq(A,y)[0]\",u_3)\n",
    "\n",
    "print(\"The residual of the method of gradient descent after 100000 iterations is\",resiValue(u_1,x,y,m,n))\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 100000 iterations is\",resiValue(u_2,x,y,m,n))\n",
    "print(\"The residual of using solution obtained with np.linalg.lstsq(A,y)[0]\",resiValue(u_3,x,y,m,n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives the smallest residual when we use solution obtained with np.linalg.lstsq(A,y)[0] as the estimated coefficients. When the number of iterations is small (1,000) or moderate (10,000), the residual of gradient descent method is larger than the residual of Nestorov method. When the number of iteration gets larger (100,000),  the residual of gradient descent method gets smaller than the residual of Nestorov method. Hence, in terms of residuals, Nestorov method method performs better in the beginning when the number of iteration is relatively small. As the number of iteration increases, gradient descent method performs better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Regularised Polynomial Regression \n",
    "\n",
    "When $m \\leq n$ (i.e. we are dealing with an underdetermined least squares problem), the minimisation problem in Question 1.1 is convex.\n",
    "\n",
    "Consider now the regularised minimisation problem\n",
    "\n",
    "$$\n",
    "u^* = \\text{argmin} \\frac{1}{2} \\|A u - y\\|_2^2 + \\frac{\\lambda}{2} \\| u\\|_2^2,\n",
    "$$\n",
    "\n",
    "which is strongly convex, and hence has a unique solution $u^*$, for any choice of regularisation parameter $\\lambda > 0$.\n",
    "\n",
    "### 2.1\n",
    "\n",
    "Show that the gradient of $\\frac{1}{2} \\|A u - y\\|_2^2 + \\frac{\\lambda}{2} \\| u\\|_2^2$ with respect to $u$ is \n",
    "\n",
    "$$(A^\\mathrm T A + \\lambda I) u - A^\\mathrm T y.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since derivatives are linear,\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2+\\frac{\\lambda}{2}\\left\\Vert u\\right\\Vert^2_2\\Big)\n",
    "    =\\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\Big)+\\lambda\\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert u\\right\\Vert^2_2\\Big).\n",
    "\\end{equation}\n",
    "From Question 1.2, we have \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u}\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2=\\mathbf{A^T}\\mathbf{A}u-\\mathbf{A^T}y,\n",
    "\\end{equation}\n",
    "and $\\frac{1}{2}\\left\\Vert u\\right\\Vert^2_2$ is a special case when $m=n$, $\\mathbf{A}=\\mathbb{1}$ and $y=0$, so\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert u\\right\\Vert^2_2\\Big)\n",
    "    &=\\Bigg[\\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\Big)\\Bigg]_{\\mathbf{A}=\\mathbb{1},y=0}\\\\\n",
    "    &=\\mathbb{1}^T\\mathbb{1}u\\\\\n",
    "    &=\\mathbb{1}u.\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "Therefore, we have \n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2+\\frac{\\lambda}{2}\\left\\Vert u\\right\\Vert^2_2\\Big)\n",
    "    &=\\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert\\mathbf{A}u-y\\right\\Vert^2_2\\Big)+\\lambda\\frac{\\partial}{\\partial u}\\Big(\\frac{1}{2}\\left\\Vert u\\right\\Vert^2_2\\Big)\\\\\n",
    "    &=\\mathbf{A^T}\\mathbf{A}u-\\mathbf{A^T}y+\\lambda\\mathbb{1}u\\\\\n",
    "    &=(\\mathbf{A^T}\\mathbf{A}+\\lambda\\mathbb{1})u-\\mathbf{A^T}y.\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "Compare the performance of gradient descent and Nestorov in computing $u^*$ in the case $\\lambda = 0$. You may want to compare your computed solution to a suitably obtained reference solution.\n",
    "\n",
    "Include any numerical tests that you run in the code cell below. Any output you use in your discussion should be displayed and easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of L is (512.0610267127927+0j)\n",
      "The value of mu is (-1.597962496517407e-14+0j)\n",
      "The minimiser from the method of gradient descent after 1000 iterations is [0.81347531+0.j 0.95854259+0.j 0.21190902+0.j 0.6124084 +0.j\n",
      " 0.40598907+0.j 0.59461731+0.j 0.54123763+0.j 0.61337079+0.j\n",
      " 0.61226184+0.j 0.62318392+0.j 0.64539801+0.j 0.62064641+0.j\n",
      " 0.65843046+0.j 0.60940509+0.j 0.66108183+0.j 0.59328261+0.j\n",
      " 0.65842308+0.j 0.57507233+0.j 0.65308891+0.j 0.55657743+0.j\n",
      " 0.64646585+0.j 0.53887144+0.j 0.63929665+0.j 0.5225387 +0.j\n",
      " 0.63198508+0.j 0.50785149+0.j 0.62475156+0.j 0.49489029+0.j\n",
      " 0.61771474+0.j 0.48362225+0.j 0.61093556+0.j 0.47395119+0.j\n",
      " 0.6044419 +0.j 0.46574896+0.j 0.59824264+0.j 0.45887471+0.j\n",
      " 0.59233591+0.j 0.45318645+0.j 0.58671399+0.j 0.44854776+0.j\n",
      " 0.58136615+0.j 0.44483135+0.j 0.57628033+0.j 0.44192077+0.j\n",
      " 0.57144408+0.j 0.43971086+0.j 0.56684512+0.j 0.43810761+0.j\n",
      " 0.5624716 +0.j 0.43702752+0.j 0.55831218+0.j 0.43639685+0.j\n",
      " 0.55435612+0.j 0.43615067+0.j 0.55059322+0.j 0.43623207+0.j\n",
      " 0.54701385+0.j 0.43659122+0.j 0.54360886+0.j 0.4371846 +0.j\n",
      " 0.54036958+0.j 0.43797428+0.j 0.53728777+0.j 0.43892722+0.j\n",
      " 0.53435558+0.j 0.44001471+0.j 0.53156556+0.j 0.44121181+0.j\n",
      " 0.5289106 +0.j 0.4424969 +0.j 0.52638394+0.j 0.44385127+0.j\n",
      " 0.52397913+0.j 0.44525872+0.j 0.52169003+0.j 0.44670531+0.j\n",
      " 0.51951082+0.j 0.44817899+0.j 0.51743594+0.j 0.44966944+0.j\n",
      " 0.5154601 +0.j 0.4511678 +0.j 0.51357828+0.j 0.45266651+0.j\n",
      " 0.51178571+0.j 0.45415912+0.j 0.51007788+0.j 0.45564019+0.j\n",
      " 0.50845048+0.j 0.45710512+0.j 0.50689944+0.j 0.45855004+0.j\n",
      " 0.50542091+0.j 0.45997178+0.j 0.50401121+0.j 0.46136769+0.j\n",
      " 0.50266689+0.j 0.46273563+0.j 0.50138467+0.j 0.4640739 +0.j\n",
      " 0.50016144+0.j 0.46538116+0.j 0.49899425+0.j 0.4666564 +0.j\n",
      " 0.49788032+0.j 0.46789887+0.j 0.49681702+0.j 0.4691081 +0.j\n",
      " 0.49580184+0.j 0.47028379+0.j 0.49483244+0.j 0.47142583+0.j\n",
      " 0.49390657+0.j 0.47253428+0.j 0.49302211+0.j 0.47360932+0.j\n",
      " 0.49217707+0.j 0.47465123+0.j 0.49136955+0.j 0.47566041+0.j\n",
      " 0.49059775+0.j 0.4766373 +0.j 0.48985996+0.j 0.47758245+0.j\n",
      " 0.48915458+0.j 0.47849644+0.j 0.48848008+0.j 0.47937989+0.j\n",
      " 0.487835  +0.j 0.48023347+0.j 0.48721797+0.j 0.48105786+0.j\n",
      " 0.48662769+0.j 0.48185379+0.j 0.48606292+0.j 0.48262198+0.j\n",
      " 0.48552248+0.j 0.48336316+0.j 0.48500526+0.j 0.48407809+0.j\n",
      " 0.48451021+0.j 0.4847675 +0.j 0.4840363 +0.j 0.48543214+0.j\n",
      " 0.4835826 +0.j 0.48607275+0.j 0.48314818+0.j 0.48669007+0.j\n",
      " 0.48273219+0.j 0.48728482+0.j 0.4823338 +0.j 0.48785771+0.j\n",
      " 0.48195223+0.j 0.48840945+0.j 0.48158674+0.j 0.48894074+0.j\n",
      " 0.48123661+0.j 0.48945224+0.j 0.48090117+0.j 0.48994463+0.j\n",
      " 0.48057977+0.j 0.49041856+0.j 0.48027181+0.j 0.49087464+0.j\n",
      " 0.4799767 +0.j 0.49131351+0.j 0.47969388+0.j 0.49173576+0.j\n",
      " 0.47942282+0.j 0.49214198+0.j 0.47916301+0.j 0.49253273+0.j\n",
      " 0.47891398+0.j 0.49290857+0.j 0.47867525+0.j 0.49327003+0.j\n",
      " 0.47844639+0.j 0.49361764+0.j 0.47822697+0.j 0.49395189+0.j\n",
      " 0.4780166 +0.j 0.49427328+0.j 0.4778149 +0.j 0.49458228+0.j\n",
      " 0.47762149+0.j 0.49487933+0.j 0.47743602+0.j 0.4951649 +0.j\n",
      " 0.47725817+0.j 0.49543939+0.j 0.4770876 +0.j 0.49570324+0.j\n",
      " 0.47692402+0.j 0.49595683+0.j 0.47676713+0.j 0.49620054+0.j\n",
      " 0.47661666+0.j 0.49643477+0.j 0.47647232+0.j 0.49665985+0.j\n",
      " 0.47633388+0.j 0.49687614+0.j 0.47620108+0.j 0.49708398+0.j\n",
      " 0.47607369+0.j 0.49728368+0.j 0.47595149+0.j 0.49747556+0.j\n",
      " 0.47583426+0.j 0.49765991+0.j 0.47572179+0.j 0.49783702+0.j\n",
      " 0.47561389+0.j 0.49800718+0.j 0.47551037+0.j 0.49817065+0.j\n",
      " 0.47541105+0.j 0.49832769+0.j 0.47531576+0.j 0.49847854+0.j\n",
      " 0.47522433+0.j 0.49862345+0.j 0.4751366 +0.j 0.49876265+0.j\n",
      " 0.47505242+0.j 0.49889636+0.j 0.47497165+0.j 0.49902478+0.j\n",
      " 0.47489415+0.j 0.49914814+0.j 0.47481978+0.j 0.49926662+0.j\n",
      " 0.47474841+0.j 0.49938041+0.j 0.47467992+0.j 0.49948971+0.j\n",
      " 0.47461421+0.j 0.49959468+0.j 0.47455114+0.j 0.49969549+0.j\n",
      " 0.47449062+0.j 0.49979231+0.j 0.47443253+0.j 0.49988529+0.j\n",
      " 0.47437679+0.j 0.49997458+0.j 0.4743233 +0.j 0.50006034+0.j\n",
      " 0.47427196+0.j 0.50014269+0.j 0.47422269+0.j 0.50022177+0.j\n",
      " 0.4741754 +0.j 0.50029772+0.j 0.47413002+0.j 0.50037064+0.j\n",
      " 0.47408646+0.j 0.50044068+0.j 0.47404465+0.j 0.50050792+0.j\n",
      " 0.47400453+0.j 0.5005725 +0.j 0.47396602+0.j 0.50063451+0.j\n",
      " 0.47392906+0.j 0.50069405+0.j 0.47389358+0.j 0.50075122+0.j\n",
      " 0.47385953+0.j 0.50080612+0.j 0.47382685+0.j 0.50085883+0.j\n",
      " 0.47379548+0.j 0.50090945+0.j 0.47376537+0.j 0.50095805+0.j\n",
      " 0.47373647+0.j 0.50100472+0.j 0.47370873+0.j 0.50104952+0.j\n",
      " 0.47368211+0.j 0.50109254+0.j 0.47365655+0.j 0.50113385+0.j\n",
      " 0.47363202+0.j 0.50117351+0.j 0.47360847+0.j 0.5012116 +0.j\n",
      " 0.47358587+0.j 0.50124816+0.j 0.47356417+0.j 0.50128326+0.j\n",
      " 0.47354335+0.j 0.50131697+0.j 0.47352336+0.j 0.50134933+0.j\n",
      " 0.47350417+0.j 0.5013804 +0.j 0.47348575+0.j 0.50141024+0.j\n",
      " 0.47346807+0.j 0.50143888+0.j 0.47345109+0.j 0.50146638+0.j\n",
      " 0.4734348 +0.j 0.50149278+0.j 0.47341916+0.j 0.50151813+0.j\n",
      " 0.47340415+0.j 0.50154247+0.j 0.47338974+0.j 0.50156583+0.j\n",
      " 0.47337591+0.j 0.50158827+0.j 0.47336263+0.j 0.5016098 +0.j\n",
      " 0.47334989+0.j 0.50163048+0.j 0.47333765+0.j 0.50165034+0.j\n",
      " 0.4733259 +0.j 0.5016694 +0.j 0.47331463+0.j 0.50168769+0.j\n",
      " 0.47330381+0.j 0.50170526+0.j 0.47329342+0.j 0.50172213+0.j\n",
      " 0.47328344+0.j 0.50173832+0.j 0.47327387+0.j 0.50175387+0.j\n",
      " 0.47326468+0.j 0.50176879+0.j 0.47325585+0.j 0.50178312+0.j\n",
      " 0.47324738+0.j 0.50179688+0.j 0.47323925+0.j 0.50181009+0.j\n",
      " 0.47323145+0.j 0.50182277+0.j 0.47322396+0.j 0.50183494+0.j\n",
      " 0.47321676+0.j 0.50184663+0.j 0.47320986+0.j 0.50185785+0.j\n",
      " 0.47320323+0.j 0.50186862+0.j 0.47319687+0.j 0.50187896+0.j\n",
      " 0.47319076+0.j 0.50188889+0.j 0.4731849 +0.j 0.50189842+0.j\n",
      " 0.47317927+0.j 0.50190757+0.j 0.47317386+0.j 0.50191635+0.j\n",
      " 0.47316868+0.j 0.50192478+0.j 0.4731637 +0.j 0.50193288+0.j\n",
      " 0.47315892+0.j 0.50194065+0.j 0.47315433+0.j 0.50194812+0.j\n",
      " 0.47314992+0.j 0.50195528+0.j 0.47314569+0.j 0.50196216+0.j\n",
      " 0.47314163+0.j 0.50196876+0.j 0.47313774+0.j 0.5019751 +0.j\n",
      " 0.47313399+0.j 0.50198118+0.j 0.4731304 +0.j 0.50198702+0.j\n",
      " 0.47312695+0.j 0.50199263+0.j 0.47312364+0.j 0.50199802+0.j\n",
      " 0.47312047+0.j 0.50200319+0.j 0.47311742+0.j 0.50200815+0.j\n",
      " 0.47311449+0.j 0.50201291+0.j 0.47311168+0.j 0.50201749+0.j\n",
      " 0.47310898+0.j 0.50202188+0.j 0.47310639+0.j 0.50202609+0.j\n",
      " 0.4731039 +0.j 0.50203014+0.j 0.47310151+0.j 0.50203402+0.j\n",
      " 0.47309922+0.j 0.50203775+0.j 0.47309702+0.j 0.50204133+0.j\n",
      " 0.47309491+0.j 0.50204477+0.j 0.47309288+0.j 0.50204807+0.j\n",
      " 0.47309093+0.j 0.50205124+0.j 0.47308906+0.j 0.50205428+0.j\n",
      " 0.47308727+0.j 0.5020572 +0.j 0.47308555+0.j 0.50206   +0.j\n",
      " 0.47308389+0.j 0.50206269+0.j 0.47308231+0.j 0.50206527+0.j\n",
      " 0.47308078+0.j 0.50206775+0.j 0.47307932+0.j 0.50207014+0.j\n",
      " 0.47307792+0.j 0.50207242+0.j 0.47307657+0.j 0.50207461+0.j\n",
      " 0.47307527+0.j 0.50207672+0.j 0.47307403+0.j 0.50207874+0.j\n",
      " 0.47307284+0.j 0.50208068+0.j 0.47307169+0.j 0.50208255+0.j\n",
      " 0.47307059+0.j 0.50208434+0.j 0.47306954+0.j 0.50208605+0.j\n",
      " 0.47306853+0.j 0.5020877 +0.j 0.47306755+0.j 0.50208929+0.j\n",
      " 0.47306662+0.j 0.50209081+0.j 0.47306572+0.j 0.50209226+0.j\n",
      " 0.47306486+0.j 0.50209367+0.j 0.47306404+0.j 0.50209501+0.j\n",
      " 0.47306324+0.j 0.5020963 +0.j 0.47306248+0.j 0.50209754+0.j\n",
      " 0.47306175+0.j 0.50209873+0.j 0.47306105+0.j 0.50209987+0.j\n",
      " 0.47306038+0.j 0.50210097+0.j 0.47305973+0.j 0.50210202+0.j\n",
      " 0.47305911+0.j 0.50210303+0.j 0.47305851+0.j 0.502104  +0.j\n",
      " 0.47305794+0.j 0.50210493+0.j 0.47305739+0.j 0.50210583+0.j\n",
      " 0.47305687+0.j 0.50210668+0.j 0.47305636+0.j 0.50210751+0.j\n",
      " 0.47305587+0.j 0.5021083 +0.j 0.47305541+0.j 0.50210906+0.j\n",
      " 0.47305496+0.j 0.50210979+0.j 0.47305453+0.j 0.50211049+0.j\n",
      " 0.47305412+0.j 0.50211116+0.j 0.47305372+0.j 0.5021118 +0.j\n",
      " 0.47305334+0.j 0.50211242+0.j 0.47305298+0.j 0.50211302+0.j\n",
      " 0.47305262+0.j 0.50211359+0.j 0.47305229+0.j 0.50211414+0.j\n",
      " 0.47305197+0.j 0.50211466+0.j 0.47305165+0.j 0.50211517+0.j\n",
      " 0.47305136+0.j]\n",
      "The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is [ 8.00038545e-01+3.50546876e-08j  8.63966819e-01+3.11816343e-07j\n",
      "  6.07971893e-01-9.26969450e-07j  1.54015665e+00-2.20279045e-06j\n",
      " -1.04917864e+00+3.79933254e-06j -4.02796859e-01+1.76303828e-06j\n",
      "  1.01261088e+00-1.78963197e-06j -4.02745777e-01+2.34480075e-06j\n",
      "  1.83972515e+00-3.77297217e-06j  2.59352227e-01+1.14961319e-06j\n",
      "  1.50597284e+00-2.29747005e-06j  8.98376822e-01-3.16644429e-07j\n",
      "  8.49128261e-01-2.22488762e-08j  1.28819354e+00-1.34507422e-06j\n",
      "  3.02207592e-01+1.71808435e-06j  1.42562766e+00-1.80955635e-06j\n",
      " -1.88518734e-02+2.62108285e-06j  1.38013453e+00-1.82648824e-06j\n",
      " -1.36682734e-01+2.81967175e-06j  1.22635892e+00-1.56110455e-06j\n",
      " -1.11578791e-01+2.54893232e-06j  1.02233411e+00-1.15281708e-06j\n",
      " -7.83512725e-04+2.01664254e-06j  8.06989090e-01-6.99068481e-07j\n",
      "  1.52677834e-01+1.37231002e-06j  6.03813159e-01-2.60473344e-07j\n",
      "  3.19847988e-01+7.12470678e-07j  4.25395865e-01+1.29278829e-07j\n",
      "  4.82601069e-01+9.46254661e-08j  2.77189721e-01+4.54484332e-07j\n",
      "  6.30349657e-01-4.49939927e-07j  1.60233435e-01+7.10741387e-07j\n",
      "  7.57505809e-01-9.06720479e-07j  7.29989152e-02+9.00342475e-07j\n",
      "  8.61716016e-01-1.27165953e-06j  1.25962009e-02+1.02923549e-06j\n",
      "  9.42685191e-01-1.54706597e-06j -2.44642815e-02+1.10508197e-06j\n",
      "  1.00140205e+00-1.73901196e-06j -4.18053454e-02+1.13605513e-06j\n",
      "  1.03962607e+00-1.85566482e-06j -4.29106680e-02+1.13012684e-06j\n",
      "  1.05954337e+00-1.90620271e-06j -3.09775442e-02+1.09467244e-06j\n",
      "  1.06353325e+00-1.90010044e-06j -8.84883305e-03+1.03627874e-06j\n",
      "  1.05400929e+00-1.84665719e-06j  2.10062779e-02+9.60678607e-07j\n",
      "  1.03331234e+00-1.75468840e-06j  5.64826036e-02+8.72761479e-07j\n",
      "  1.00364055e+00-1.63233308e-06j  9.58132941e-02+7.76626869e-07j\n",
      "  9.67006622e-01-1.48694406e-06j  1.37536391e-01+6.75659462e-07j\n",
      "  9.25214701e-01-1.32503872e-06j  1.80458728e-01+5.72612488e-07j\n",
      "  8.79851893e-01-1.15229327e-06j  2.23620243e-01+4.69691272e-07j\n",
      "  8.32290035e-01-9.73567918e-07j  2.66260432e-01+3.68632310e-07j\n",
      "  7.83694662e-01-7.92952998e-07j  3.07787819e-01+2.70775483e-07j\n",
      "  7.35038669e-01-6.13828462e-07j  3.47752794e-01+1.77128417e-07j\n",
      "  6.87118797e-01-4.38930796e-07j  3.85823817e-01+8.84228400e-08j\n",
      "  6.40573525e-01-2.70422965e-07j  4.21766836e-01+5.16331120e-09j\n",
      "  5.95901336e-01-1.09964146e-07j  4.55427641e-01-7.23310745e-08j\n",
      "  5.53478594e-01+4.12230459e-08j  4.86716855e-01-1.43891264e-07j\n",
      "  5.13576551e-01+1.82289365e-07j  5.15597244e-01-2.09468359e-07j\n",
      "  4.76377151e-01+3.12700737e-07j  5.42073054e-01-2.69108540e-07j\n",
      "  4.41987443e-01+4.32185526e-07j  5.66181079e-01-3.22932123e-07j\n",
      "  4.10452530e-01+5.40687453e-07j  5.87983231e-01-3.71116163e-07j\n",
      "  3.81767053e-01+6.38324083e-07j  6.07560385e-01-4.13880059e-07j\n",
      "  3.55885247e-01+7.25350632e-07j  6.25007291e-01-4.51473715e-07j\n",
      "  3.32729647e-01+8.02128768e-07j  6.40428416e-01-4.84167842e-07j\n",
      "  3.12198568e-01+8.69099975e-07j  6.53934537e-01-5.12246074e-07j\n",
      "  2.94172438e-01+9.26763059e-07j  6.65639998e-01-5.35998599e-07j\n",
      "  2.78519130e-01+9.75655347e-07j  6.75660495e-01-5.55717067e-07j\n",
      "  2.65098395e-01+1.01633714e-06j  6.84111322e-01-5.71690551e-07j\n",
      "  2.53765502e-01+1.04937903e-06j  6.91105988e-01-5.84202393e-07j\n",
      "  2.44374196e-01+1.07535166e-06j  6.96755159e-01-5.93527793e-07j\n",
      "  2.36779067e-01+1.09481761e-06j  7.01165845e-01-5.99931992e-07j\n",
      "  2.30837417e-01+1.10832512e-06j  7.04440809e-01-6.03668965e-07j\n",
      "  2.26410688e-01+1.11640330e-06j  7.06678158e-01-6.04980516e-07j\n",
      "  2.23365548e-01+1.11955858e-06j  7.07971061e-01-6.04095723e-07j\n",
      "  2.21574660e-01+1.11827233e-06j  7.08407601e-01-6.01230635e-07j\n",
      "  2.20917217e-01+1.11299925e-06j  7.08070712e-01-5.96588200e-07j\n",
      "  2.21279265e-01+1.10416648e-06j  7.07038188e-01-5.90358366e-07j\n",
      "  2.22553861e-01+1.09217337e-06j  7.05382759e-01-5.82718311e-07j\n",
      "  2.24641110e-01+1.07739155e-06j  7.03172209e-01-5.73832784e-07j\n",
      "  2.27448085e-01+1.06016553e-06j  7.00469525e-01-5.63854525e-07j\n",
      "  2.30888672e-01+1.04081339e-06j  6.97333080e-01-5.52924738e-07j\n",
      "  2.34883357e-01+1.01962784e-06j  6.93816827e-01-5.41173612e-07j\n",
      "  2.39358965e-01+9.96877282e-07j  6.89970509e-01-5.28720864e-07j\n",
      "  2.44248364e-01+9.72807069e-07j  6.85839874e-01-5.15676293e-07j\n",
      "  2.49490157e-01+9.47640769e-07j  6.81466897e-01-5.02140346e-07j\n",
      "  2.55028355e-01+9.21581473e-07j  6.76889997e-01-4.88204674e-07j\n",
      "  2.60812046e-01+8.94813106e-07j  6.72144254e-01-4.73952688e-07j\n",
      "  2.66795072e-01+8.67501722e-07j  6.67261624e-01-4.59460092e-07j\n",
      "  2.72935700e-01+8.39796769e-07j  6.62271142e-01-4.44795406e-07j\n",
      "  2.79196311e-01+8.11832310e-07j  6.57199122e-01-4.30020467e-07j\n",
      "  2.85543097e-01+7.83728199e-07j  6.52069351e-01-4.15190913e-07j\n",
      "  2.91945766e-01+7.55591200e-07j  6.46903267e-01-4.00356635e-07j\n",
      "  2.98377272e-01+7.27516050e-07j  6.41720136e-01-3.85562213e-07j\n",
      "  3.04813546e-01+6.99586457e-07j  6.36537215e-01-3.70847333e-07j\n",
      "  3.11233255e-01+6.71876042e-07j  6.31369907e-01-3.56247168e-07j\n",
      "  3.17617564e-01+6.44449220e-07j  6.26231910e-01-3.41792744e-07j\n",
      "  3.23949928e-01+6.17362017e-07j  6.21135349e-01-3.27511286e-07j\n",
      "  3.30215880e-01+5.90662840e-07j  6.16090912e-01-3.13426534e-07j\n",
      "  3.36402851e-01+5.64393177e-07j  6.11107963e-01-2.99559041e-07j\n",
      "  3.42499992e-01+5.38588255e-07j  6.06194662e-01-2.85926456e-07j\n",
      "  3.48498014e-01+5.13277644e-07j  6.01358061e-01-2.72543778e-07j\n",
      "  3.54389039e-01+4.88485812e-07j  5.96604211e-01-2.59423597e-07j\n",
      "  3.60166464e-01+4.64232638e-07j  5.91938242e-01-2.46576322e-07j\n",
      "  3.65824834e-01+4.40533881e-07j  5.87364456e-01-2.34010377e-07j\n",
      "  3.71359726e-01+4.17401606e-07j  5.82886399e-01-2.21732403e-07j\n",
      "  3.76767645e-01+3.94844585e-07j  5.78506932e-01-2.09747425e-07j\n",
      "  3.82045925e-01+3.72868647e-07j  5.74228303e-01-1.98059016e-07j\n",
      "  3.87192642e-01+3.51477011e-07j  5.70052200e-01-1.86669449e-07j\n",
      "  3.92206531e-01+3.30670584e-07j  5.65979813e-01-1.75579831e-07j\n",
      "  3.97086915e-01+3.10448228e-07j  5.62011884e-01-1.64790229e-07j\n",
      "  4.01833636e-01+2.90807013e-07j  5.58148753e-01-1.54299787e-07j\n",
      "  4.06446995e-01+2.71742432e-07j  5.54390402e-01-1.44106827e-07j\n",
      "  4.10927697e-01+2.53248610e-07j  5.50736496e-01-1.34208953e-07j\n",
      "  4.15276798e-01+2.35318484e-07j  5.47186417e-01-1.24603131e-07j\n",
      "  4.19495663e-01+2.17943969e-07j  5.43739302e-01-1.15285775e-07j\n",
      "  4.23585924e-01+2.01116107e-07j  5.40394066e-01-1.06252816e-07j\n",
      "  4.27549442e-01+1.84825198e-07j  5.37149437e-01-9.74997720e-08j\n",
      "  4.31388272e-01+1.69060928e-07j  5.34003977e-01-8.90218075e-08j\n",
      "  4.35104639e-01+1.53812470e-07j  5.30956107e-01-8.08137861e-08j\n",
      "  4.38700902e-01+1.39068584e-07j  5.28004124e-01-7.28703225e-08j\n",
      "  4.42179538e-01+1.24817706e-07j  5.25146225e-01-6.51858266e-08j\n",
      "  4.45543113e-01+1.11048022e-07j  5.22380523e-01-5.77545433e-08j\n",
      "  4.48794269e-01+9.77475384e-08j  5.19705057e-01-5.05705896e-08j\n",
      "  4.51935702e-01+8.49041441e-08j  5.17117815e-01-4.36279873e-08j\n",
      "  4.54970148e-01+7.25056637e-08j  5.14616738e-01-3.69206916e-08j\n",
      "  4.57900368e-01+6.05399070e-08j  5.12199736e-01-3.04426184e-08j\n",
      "  4.60729140e-01+4.89947099e-08j  5.09864697e-01-2.41876664e-08j\n",
      "  4.63459241e-01+3.78579724e-08j  5.07609497e-01-1.81497389e-08j\n",
      "  4.66093445e-01+2.71176914e-08j  5.05432006e-01-1.23227621e-08j\n",
      "  4.68634510e-01+1.67619875e-08j  5.03330095e-01-6.70070057e-09j\n",
      "  4.71085170e-01+6.77913153e-09j  5.01301647e-01-1.27757261e-09j\n",
      "  4.73448132e-01-2.84243623e-09j  4.99344555e-01+3.95253806e-09j\n",
      "  4.75726070e-01-1.21140871e-08j  4.97456734e-01+8.99547104e-09j\n",
      "  4.77921617e-01-2.10469896e-08j  4.95636122e-01+1.38569795e-08j\n",
      "  4.80037363e-01-2.96520957e-08j  4.93880682e-01+1.85427232e-08j\n",
      "  4.82075855e-01-3.79401316e-08j  4.92188409e-01+2.30582604e-08j\n",
      "  4.84039587e-01-4.59215893e-08j  4.90557331e-01+2.74090430e-08j\n",
      "  4.85931003e-01-5.36067187e-08j  4.88985512e-01+3.16004113e-08j\n",
      "  4.87752494e-01-6.10055236e-08j  4.87471051e-01+3.56375898e-08j\n",
      "  4.89506396e-01-6.81277584e-08j  4.86012089e-01+3.95256854e-08j\n",
      "  4.91194987e-01-7.49829248e-08j  4.84606806e-01+4.32696834e-08j\n",
      "  4.92820493e-01-8.15802714e-08j  4.83253422e-01+4.68744473e-08j\n",
      "  4.94385076e-01-8.79287927e-08j  4.81950202e-01+5.03447162e-08j\n",
      "  4.95890846e-01-9.40372310e-08j  4.80695451e-01+5.36851059e-08j\n",
      "  4.97339852e-01-9.99140761e-08j  4.79487519e-01+5.69001067e-08j\n",
      "  4.98734087e-01-1.05567569e-07j  4.78324800e-01+5.99940856e-08j\n",
      "  5.00075487e-01-1.11005702e-07j  4.77205728e-01+6.29712856e-08j\n",
      "  5.01365928e-01-1.16236227e-07j  4.76128785e-01+6.58358273e-08j\n",
      "  5.02607235e-01-1.21266652e-07j  4.75092491e-01+6.85917097e-08j\n",
      "  5.03801172e-01-1.26104250e-07j  4.74095414e-01+7.12428119e-08j\n",
      "  5.04949453e-01-1.30756065e-07j  4.73136160e-01+7.37928947e-08j\n",
      "  5.06053736e-01-1.35228907e-07j  4.72213378e-01+7.62456022e-08j\n",
      "  5.07115626e-01-1.39529370e-07j  4.71325760e-01+7.86044645e-08j\n",
      "  5.08136677e-01-1.43663825e-07j  4.70472037e-01+8.08728987e-08j\n",
      "  5.09118392e-01-1.47638433e-07j  4.69650980e-01+8.30542124e-08j\n",
      "  5.10062224e-01-1.51459146e-07j  4.68861400e-01+8.51516045e-08j\n",
      "  5.10969577e-01-1.55131713e-07j  4.68102144e-01+8.71681693e-08j\n",
      "  5.11841810e-01-1.58661687e-07j  4.67372100e-01+8.91068975e-08j\n",
      "  5.12680233e-01-1.62054428e-07j  4.66670190e-01+9.09706797e-08j\n",
      "  5.13486113e-01-1.65315108e-07j  4.65995373e-01+9.27623081e-08j\n",
      "  5.14260672e-01-1.68448720e-07j  4.65346643e-01+9.44844799e-08j\n",
      "  5.15005091e-01-1.71460078e-07j  4.64723028e-01+9.61397985e-08j\n",
      "  5.15720508e-01-1.74353827e-07j  4.64123589e-01+9.77307781e-08j\n",
      "  5.16408021e-01-1.77134444e-07j  4.63547420e-01+9.92598441e-08j\n",
      "  5.17068690e-01-1.79806247e-07j  4.62993646e-01+1.00729337e-07j\n",
      "  5.17703537e-01-1.82373396e-07j  4.62461422e-01+1.02141514e-07j\n",
      "  5.18313546e-01-1.84839902e-07j  4.61949934e-01+1.03498552e-07j\n",
      "  5.18899666e-01-1.87209627e-07j  4.61458396e-01+1.04802551e-07j\n",
      "  5.19462813e-01-1.89486293e-07j  4.60986051e-01+1.06055534e-07j\n",
      "  5.20003868e-01-1.91673486e-07j  4.60532167e-01+1.07259451e-07j\n",
      "  5.20523681e-01-1.93774658e-07j  4.60096040e-01+1.08416182e-07j\n",
      "  5.21023068e-01-1.95793133e-07j  4.59676991e-01+1.09527538e-07j\n",
      "  5.21502819e-01-1.97732112e-07j  4.59274367e-01+1.10595263e-07j\n",
      "  5.21963692e-01-1.99594677e-07j  4.58887536e-01+1.11621038e-07j\n",
      "  5.22406416e-01-2.01383792e-07j  4.58515892e-01+1.12606480e-07j\n",
      "  5.22831696e-01-2.03102314e-07j  4.58158849e-01+1.13553148e-07j\n",
      "  5.23240208e-01-2.04752987e-07j  4.57815846e-01+1.14462542e-07j\n",
      "  5.23632604e-01-2.06338457e-07j  4.57486338e-01+1.15336106e-07j\n",
      "  5.24009510e-01-2.07861265e-07j  4.57169804e-01+1.16175231e-07j\n",
      "  5.24371530e-01-2.09323859e-07j  4.56865742e-01+1.16981255e-07j\n",
      "  5.24719244e-01-2.10728591e-07j  4.56573666e-01+1.17755464e-07j\n",
      "  5.25053210e-01-2.12077725e-07j  4.56293113e-01+1.18499097e-07j\n",
      "  5.25373967e-01-2.13373439e-07j  4.56023633e-01+1.19213348e-07j\n",
      "  5.25682029e-01-2.14617826e-07j  4.55764796e-01+1.19899362e-07j\n",
      "  5.25977895e-01-2.15812900e-07j  4.55516186e-01+1.20558244e-07j\n",
      "  5.26262043e-01-2.16960596e-07j  4.55277404e-01+1.21191052e-07j\n",
      "  5.26534931e-01-2.18062776e-07j  4.55048066e-01+1.21798810e-07j\n",
      "  5.26797002e-01-2.19121230e-07j  4.54827803e-01+1.22382497e-07j\n",
      "  5.27048681e-01-2.20137679e-07j  4.54616261e-01+1.22943059e-07j\n",
      "  5.27290376e-01-2.21113776e-07j  4.54413096e-01+1.23481401e-07j\n",
      "  5.27522480e-01-2.22051111e-07j  4.54217980e-01+1.23998398e-07j\n",
      "  5.27745370e-01-2.22951213e-07j  4.54030598e-01+1.24494889e-07j\n",
      "  5.27959411e-01-2.23815551e-07j  4.53850647e-01+1.24971679e-07j\n",
      "  5.28164949e-01-2.24645535e-07j  4.53677832e-01+1.25429544e-07j\n",
      "  5.28362322e-01-2.25442523e-07j  4.53511875e-01+1.25869231e-07j\n",
      "  5.28551850e-01-2.26207819e-07j  4.53352505e-01+1.26291454e-07j\n",
      "  5.28733845e-01-2.26942676e-07j  4.53199463e-01+1.26696904e-07j\n",
      "  5.28908603e-01-2.27648298e-07j  4.53052499e-01+1.27086241e-07j\n",
      "  5.29076410e-01-2.28325841e-07j  4.52911373e-01+1.27460103e-07j\n",
      "  5.29237542e-01-2.28976417e-07j  4.52775856e-01+1.27819100e-07j\n",
      "  5.29392263e-01-2.29601095e-07j  4.52645725e-01+1.28163820e-07j\n",
      "  5.29540825e-01-2.30200900e-07j  4.52520769e-01+1.28494828e-07j\n",
      "  5.29683474e-01-2.30776818e-07j  4.52400781e-01+1.28812665e-07j\n",
      "  5.29820444e-01-2.31329798e-07j  4.52285567e-01+1.29117854e-07j\n",
      "  5.29951959e-01-2.31860747e-07j  4.52174936e-01+1.29410896e-07j\n",
      "  5.30078236e-01-2.32370542e-07j  4.52068709e-01+1.29692270e-07j\n",
      "  5.30199483e-01-2.32860021e-07j  4.51966709e-01+1.29962441e-07j\n",
      "  5.30315899e-01-2.33329991e-07j  4.51868771e-01+1.30221851e-07j\n",
      "  5.30427676e-01-2.33781227e-07j  4.51774732e-01+1.30470929e-07j\n",
      "  5.30534998e-01-2.34214473e-07j  4.51684438e-01+1.30710083e-07j\n",
      "  5.30638043e-01-2.34630445e-07j  4.51597741e-01+1.30939707e-07j\n",
      "  5.30736979e-01-2.35029828e-07j  4.51514499e-01+1.31160180e-07j\n",
      "  5.30831970e-01-2.35413283e-07j  4.51434573e-01+1.31371866e-07j\n",
      "  5.30923174e-01-2.35781443e-07j  4.51357832e-01+1.31575113e-07j\n",
      "  5.31010740e-01-2.36134917e-07j  4.51284150e-01+1.31770256e-07j\n",
      "  5.31094814e-01-2.36474288e-07j  4.51213406e-01+1.31957618e-07j\n",
      "  5.31175534e-01-2.36800118e-07j  4.51145482e-01+1.32137508e-07j\n",
      "  5.31253034e-01-2.37112946e-07j  4.51080266e-01+1.32310224e-07j\n",
      "  5.31327441e-01-2.37413290e-07j  4.51017651e-01+1.32476050e-07j\n",
      "  5.31398879e-01-2.37701646e-07j  4.50957534e-01+1.32635260e-07j\n",
      "  5.31467467e-01-2.37978493e-07j  4.50899815e-01+1.32788118e-07j\n",
      "  5.31533317e-01-2.38244289e-07j  4.50844398e-01+1.32934878e-07j\n",
      "  5.31596539e-01-2.38499474e-07j  4.50791192e-01+1.33075780e-07j\n",
      "  5.31657237e-01-2.38744470e-07j  4.50740109e-01+1.33211059e-07j\n",
      "  5.31715512e-01-2.38979685e-07j  4.50691065e-01+1.33340939e-07j\n",
      "  5.31771460e-01-2.39205507e-07j  4.50643978e-01+1.33465635e-07j\n",
      "  5.31825175e-01-2.39422312e-07j  4.50598771e-01+1.33585353e-07j\n",
      "  5.31876745e-01-2.39630458e-07j  4.50555367e-01+1.33700291e-07j\n",
      "  5.31926255e-01-2.39830291e-07j  4.50513697e-01+1.33810640e-07j\n",
      "  5.31973789e-01-2.40022143e-07j  4.50473690e-01+1.33916584e-07j\n",
      "  5.32019424e-01-2.40206332e-07j  4.50435281e-01+1.34018296e-07j\n",
      "  5.32063236e-01-2.40383164e-07j  4.50398405e-01+1.34115948e-07j\n",
      "  5.32105298e-01-2.40552931e-07j  4.50363001e-01+1.34209699e-07j\n",
      "  5.32145681e-01-2.40715917e-07j  4.50329011e-01+1.34299706e-07j\n",
      "  5.32184450e-01-2.40872391e-07j  4.50296379e-01+1.34386118e-07j\n",
      "  5.32221670e-01-2.41022614e-07j  4.50265050e-01+1.34469078e-07j\n",
      "  5.32257404e-01-2.41166835e-07j  4.50234973e-01+1.34548724e-07j\n",
      "  5.32291709e-01-2.41305293e-07j  4.50206097e-01+1.34625189e-07j\n",
      "  5.32324644e-01-2.41438218e-07j  4.50178374e-01+1.34698599e-07j\n",
      "  5.32356264e-01-2.41565832e-07j  4.50151758e-01+1.34769076e-07j\n",
      "  5.32386619e-01-2.41688347e-07j  4.50126206e-01+1.34836738e-07j\n",
      "  5.32415762e-01-2.41805965e-07j  4.50101675e-01+1.34901696e-07j\n",
      "  5.32443740e-01-2.41918883e-07j  4.50078124e-01+1.34964059e-07j\n",
      "  5.32470601e-01-2.42027289e-07j  4.50055514e-01+1.35023929e-07j\n",
      "  5.32496387e-01-2.42131361e-07j  4.50033807e-01+1.35081407e-07j\n",
      "  5.32521144e-01-2.42231275e-07j  4.50012968e-01+1.35136588e-07j\n",
      "  5.32544910e-01-2.42327194e-07j  4.49992961e-01+1.35189565e-07j\n",
      "  5.32567727e-01-2.42419280e-07j  4.49973754e-01+1.35240424e-07j\n",
      "  5.32589632e-01-2.42507686e-07j  4.49955315e-01+1.35289250e-07j\n",
      "  5.32610662e-01-2.42592557e-07j  4.49937612e-01+1.35336125e-07j\n",
      "  5.32630851e-01-2.42674036e-07j  4.49920617e-01+1.35381126e-07j\n",
      "  5.32650233e-01-2.42752258e-07j  4.49904301e-01+1.35424329e-07j\n",
      "  5.32668840e-01-2.42827353e-07j  4.49888638e-01+1.35465805e-07j\n",
      "  5.32686703e-01-2.42899446e-07j  4.49873600e-01+1.35505623e-07j\n",
      "  5.32703852e-01-2.42968657e-07j  4.49859163e-01+1.35543850e-07j\n",
      "  5.32720316e-01-2.43035101e-07j  4.49845304e-01+1.35580548e-07j\n",
      "  5.32736122e-01-2.43098889e-07j  4.49831998e-01+1.35615780e-07j\n",
      "  5.32751295e-01-2.43160127e-07j  4.49819224e-01+1.35649603e-07j\n",
      "  5.32765862e-01-2.43218916e-07j  4.49806961e-01+1.35682074e-07j\n",
      "  5.32779847e-01-2.43275356e-07j  4.49795188e-01+1.35713247e-07j\n",
      "  5.32793273e-01-2.43329538e-07j  4.49783886e-01+1.35743173e-07j\n",
      "  5.32806162e-01-2.43381554e-07j  4.49773036e-01+1.35771904e-07j\n",
      "  5.32818535e-01-2.43431491e-07j  4.49762619e-01+1.35799485e-07j\n",
      "  5.32830414e-01-2.43479431e-07j  4.49752619e-01+1.35825964e-07j\n",
      "  5.32841818e-01-2.43525454e-07j  4.49743018e-01+1.35851385e-07j\n",
      "  5.32852766e-01-2.43569637e-07j  4.49733802e-01+1.35875789e-07j\n",
      "  5.32863276e-01-2.43612054e-07j  4.49724953e-01+1.35899217e-07j\n",
      "  5.32873366e-01-2.43652774e-07j  4.49716459e-01+1.35921709e-07j\n",
      "  5.32883053e-01-2.43691866e-07j  4.49708304e-01+1.35943301e-07j\n",
      "  5.32892352e-01-2.43729396e-07j  4.49700476e-01+1.35964030e-07j\n",
      "  5.32901280e-01-2.43765424e-07j  4.49692960e-01+1.35983930e-07j\n",
      "  5.32909850e-01-2.43800012e-07j]\n",
      "Solution obtained with np.linalg.lstsq(A[:,0:m],y)[0] [ 8.64151020e-01  7.29756443e-01 -8.89848790e+00 -2.38213881e+01\n",
      " -1.29619961e+02  2.81296398e+03  3.34609464e+04 -8.12965905e+04\n",
      " -1.25759993e+06  1.06067926e+06  2.35678712e+07 -6.96727188e+06\n",
      " -2.65691133e+08  1.84923623e+07  1.94028025e+09  4.68555835e+07\n",
      " -9.46679510e+09 -5.51567528e+08  3.08626882e+10  1.98493706e+09\n",
      " -6.45396127e+10 -3.62323682e+09  7.49172760e+10  2.75567369e+09\n",
      " -1.80250816e+10  1.19533971e+09 -5.62130283e+10 -2.69722461e+09\n",
      "  2.73541547e+10 -9.98060590e+08  5.24349192e+10  2.04187859e+09\n",
      " -8.85226346e+09  1.68287284e+09 -5.18324198e+10 -7.32286415e+08\n",
      " -2.51176278e+10 -1.95150940e+09  2.81299109e+10 -9.54951364e+08\n",
      "  4.78442929e+10  7.69396328e+08  2.05665254e+10  1.59215244e+09\n",
      " -2.27265813e+10  1.00879212e+09 -4.46830486e+10 -2.05860965e+08\n",
      " -3.17061749e+10 -1.13924556e+09  2.66677637e+09 -1.19047773e+09\n",
      "  3.33102543e+10 -5.38387575e+08  4.16729104e+10  3.54310655e+08\n",
      "  2.52290822e+10  9.35547729e+08 -4.50718852e+09  9.65035822e+08\n",
      " -3.05416460e+10  5.09429003e+08 -3.99900789e+10 -1.32046184e+08\n",
      " -2.94349775e+10 -6.37694362e+08 -5.18623568e+09 -7.87865549e+08\n",
      "  2.08465531e+10 -5.92617060e+08  3.68290407e+10 -1.77487631e+08\n",
      "  3.61049703e+10  2.71782440e+08  1.94062149e+10  5.52001320e+08\n",
      " -5.85486761e+09  5.90074742e+08 -2.86266229e+10  3.81888742e+08\n",
      " -3.87992128e+10  3.87235555e+07 -3.11720360e+10 -3.17397842e+08\n",
      " -8.17227527e+09 -4.43554727e+08  2.03519222e+10 -3.78708536e+08\n",
      "  3.98316901e+10 -1.39218622e+08  3.68884273e+10  1.43760760e+08\n",
      "  7.00881888e+09  3.14253177e+08 -3.61526283e+10  2.46620860e+08\n",
      " -5.13944917e+10 -4.85059850e+07  3.90415341e+10 -1.55929266e+08]\n",
      "The residual of the method of gradient descent after 1000 iterations is (0.906582836704474+0j)\n",
      "The residual of Nestorov method for strongly convex objectives after 1000 iterations is (0.8580826751962342-2.64325907501949e-07j)\n",
      "The residual of using solution obtained with np.linalg.lstsq(A[:,0:m],y)[0] 0.44058655713899164\n",
      "The residual of using solution obtained with nnls(A,y)[0] 0.832790233244138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load matrix A\n",
    "A = np.load('data_A.npy')\n",
    "\n",
    "# Load corresponding data points {y_i}\n",
    "y = np.load('data_b.npy')\n",
    "\n",
    "\n",
    "# add your code here\n",
    "import scipy.linalg\n",
    "from scipy.linalg import qr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import nnls\n",
    "m=A.shape[0] #m is the number of data points\n",
    "n=A.shape[1] #n-1 is the polynomial degree\n",
    "eigen=np.linalg.eigvals(A.T.dot(A)) #eigen is the vector of eigen values of A.T.dot(A)\n",
    "L=eigen.max()  #Set the value of L the largest eigen value of A.T.dot(A)\n",
    "mu=eigen.min() #Set the value of mu the smallest eigen value of A.T.dot(A)\n",
    "print(\"The value of L is\",L)\n",
    "print(\"The value of mu is\",mu)\n",
    "\n",
    "def GDminimiser(y,A,h,u_0,num):\n",
    "    #h is step size, u_0 is initial point, num is the number of iterations \n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from gradiant descent method\n",
    "    \"\"\"\n",
    "    u=u_0  #starting point\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run num interations\n",
    "        #in each iteration\n",
    "        #claculate the gradient of the objective function with respect to u\n",
    "        delta_u=A.T.dot(A).dot(u)-A.T.dot(y) \n",
    "        #update the value of u via the gradiant method\n",
    "        u=u-delta_u*h \n",
    "        \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "def convex_min(y,A,u_0,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    k=1    #index for iteration\n",
    "    t_minus1=1  #initial value of a sequence values of t\n",
    "    t=t_minus1  #current value of t\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        t_plus=((1+4*(t**2))**(0.5)+1)/2   #calculate the new value for t\n",
    "        gamma=(t-1)/t_plus                 #calculate the value of gamma\n",
    "        t=t_plus                           #update the value of t\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=A.T.dot(A).dot(v)-A.T.dot(y) \n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "def Sconvex_min(y,A,u_0,L,mu,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for strongly convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    gamma=(L**(0.5)-mu**(0.5))/(L**(0.5)+mu**(0.5)) #calculate the constant value of gamma\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=A.T.dot(A).dot(v)-A.T.dot(y) \n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "##Set the number of iterations as 1000. \n",
    "u_0=np.zeros(n) #set the initial value of all elements of u as 0\n",
    "\n",
    "u_1=GDminimiser(y,A,1/L,u_0,1000) #use a constant step size of 1/L\n",
    "print(\"The minimiser from the method of gradient descent after 1000 iterations is\",u_1)\n",
    "\n",
    "u_2=Sconvex_min(y,A,u_0,L,mu,1000)\n",
    "print(\"The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is\",u_2)\n",
    "\n",
    "u_3=np.linalg.lstsq(A[:,0:m],y,rcond=None)[0]\n",
    "print(\"Solution obtained with np.linalg.lstsq(A[:,0:m],y)[0]\",u_3)\n",
    "\n",
    "u_4=nnls(A,y)[0] #solution obtained with nnls(A,y)[0]\n",
    "\n",
    "\n",
    "\n",
    "def resi_Value(u_estimate,A,y,m,n):\n",
    "    \"\"\"\n",
    "    Return the residuals of different methods\n",
    "    \"\"\"\n",
    "    res=0\n",
    "    for i in range(1,m): #we will sum the residuals of all m data points\n",
    "        res=res+(y[i-1]-u_estimate.dot(A[i-1]))**2\n",
    "    return res #output is the residuals\n",
    "\n",
    "print(\"The residual of the method of gradient descent after 1000 iterations is\",resi_Value(u_1,A,y,m,n))\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 1000 iterations is\",resi_Value(u_2,A,y,m,n))\n",
    "print(\"The residual of using solution obtained with np.linalg.lstsq(A[:,0:m],y)[0]\",resi_Value(u_3,A[:,0:m],y,m,n))\n",
    "print(\"The residual of using solution obtained with nnls(A,y)[0]\",resi_Value(u_4,A,y,m,n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with 1.5, residuals in 2.2 are much smaller. To obtain a reference solution, eatract the first m rows of matrix A to be new matrix, and then use np.linalg.lstsq(A,y)[0]. We can see that gradient descent method give similar coefficient estimators for every polynomial term of x. The coefficients obtained from Nestorov method for strongly convex objectives are more variant for the lower order polynomial terms and more similar for the higher order polynomial terms. The coefficients obtained from np.linalg.lstsq(A,y)[0] after slicing matrix A have very large magnitude. Both the gradient descent method and Nestorov method give similar residuals, which are also greater than the the residuals of using solution obtained with np.linalg.lstsq(A[:,0:m],y)[0]. So in terms of residuals, both methods have similar performance. However, overfitting might be a potential problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Compare the performance of gradient descent and Nestorov (both versions) in computing $u^*$ in the case $\\lambda = 0.01\\|A^T A\\|_{2}$. In this case the unique solution can be obtained by solving an appropriate augmented least squares problem. \n",
    "\n",
    "Include any numerical tests that you run in the code cell below. Any output you use in your discussion should be displayed and easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of L is (512.0610267127927+0j)\n",
      "The value of mu is (-1.597962496517407e-14+0j)\n",
      "the value of lambda is 5.120610267127922\n",
      "The residual of the method of gradient descent after 1000 iterations is (1.8288473560713514+0j)\n",
      "The residual of Nestorov method for strongly convex objectives after 1000 iterations is (49059.809178733376-0.3776792958712106j)\n",
      "The residual of Nestorov method for convex objectives after 1000 iterations is (1.8300432291179611+0j)\n",
      "The residual of the method of gradient descent after 10000 iterations is (1.828846908029326+0j)\n",
      "The residual of Nestorov method for strongly convex objectives after 10000 iterations is (4463.086357838996-1.6318141712968444j)\n",
      "The residual of Nestorov method for convex objectives after 10000 iterations is (1.8288129469819212+0j)\n",
      "The residual of Nestorov method for strongly convex objectives after 100000 iterations is (4836.5820351627635-7.477858911311769j)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load matrix A\n",
    "A = np.load('data_A.npy')\n",
    "\n",
    "# Load corresponding data points {y_i}\n",
    "y = np.load('data_b.npy')\n",
    "\n",
    "\n",
    "# add your code here\n",
    "m=A.shape[0] #m is the number of data points\n",
    "n=A.shape[1] #n-1 is the polynomial degree\n",
    "eigen=np.linalg.eigvals(A.T.dot(A)) #eigen is the vector of eigen values of A.T.dot(A)\n",
    "L=eigen.max()  #Set the value of L the largest eigen value of A.T.dot(A)\n",
    "mu=eigen.min() #Set the value of mu the smallest eigen value of A.T.dot(A)\n",
    "print(\"The value of L is\",L)\n",
    "print(\"The value of mu is\",mu)\n",
    "u_0=np.zeros(n) #set the initial value of all elements of u as 0\n",
    "lambd=0.01*np.linalg.norm(A.T.dot(A),ord=2)\n",
    "print(\"the value of lambda is\",lambd)\n",
    "def GDminimiser2(y,A,h,u_0,num):\n",
    "    #h is step size, u_0 is initial point, num is the number of iterations \n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from gradiant descent method\n",
    "    \"\"\"\n",
    "    u=u_0  #starting point\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run num interations\n",
    "        #in each iteration\n",
    "        #claculate the gradient of the objective function with respect to u\n",
    "        delta_u=(A.T.dot(A)+lambd*np.identity(n)).dot(u)-A.T.dot(y) \n",
    "        #update the value of u via the gradiant method\n",
    "        u=u-delta_u*h \n",
    "        \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "\n",
    "def Sconvex_min2(y,A,u_0,L,mu,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for strongly convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    gamma=(L**(0.5)-mu**(0.5))/(L**(0.5)+mu**(0.5)) #calculate the constant value of gamma\n",
    "    k=1    #index for iteration\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=(A.T.dot(A)+lambd*np.identity(n)).dot(u)-A.T.dot(y)\n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "def convex_min2(y,A,u_0,num):\n",
    "    #u_0 is the initial point\n",
    "    \"\"\"\n",
    "    Return the minimiser obtained from Nestorov method for convex objectives\n",
    "    \"\"\"\n",
    "    u=u_0  #set the initial value of u as u_0 \n",
    "    v=u    #v will be updated after each iteration\n",
    "    k=1    #index for iteration\n",
    "    t_minus1=1  #initial value of a sequence values of t\n",
    "    t=t_minus1  #current value of t\n",
    "    \n",
    "    for k in range(1,num): #run n interations\n",
    "        #in each iteration\n",
    "        t_plus=((1+4*(t**2))**(0.5)+1)/2   #calculate the new value for t\n",
    "        gamma=(t-1)/t_plus                 #calculate the value of gamma\n",
    "        t=t_plus                           #update the value of t\n",
    "        #calculate the gradient of the objective function at the point v\n",
    "        delta_v=(A.T.dot(A)+lambd*np.identity(n)).dot(u)-A.T.dot(y)\n",
    "        u_plus=v-delta_v/L          #calculate the new value for u\n",
    "        v=u_plus+gamma*(u_plus-u)   #update the value of v\n",
    "        u=u_plus                    #update the value of u\n",
    "    \n",
    "    return u #output is the minimiser after n iterations\n",
    "\n",
    "def resi_Value(u_estimate,A,y,m,n):\n",
    "    \"\"\"\n",
    "    Return the residuals of different methods\n",
    "    \"\"\"\n",
    "    res=0\n",
    "    for i in range(1,m): #we will sum the residuals of all m data points\n",
    "        res=res+(y[i-1]-u_estimate.dot(A[i-1]))**2\n",
    "    return res #output is the residuals\n",
    "\n",
    "##set the number of iterations as 1000\n",
    "uu_1=GDminimiser2(y,A,1/L,u_0,1000) #use a constant step size of 1/L\n",
    "#print(\"The minimiser from the method of gradient descent after 1000 iterations is\",uu_1)\n",
    "print(\"The residual of the method of gradient descent after 1000 iterations is\",resi_Value(uu_1,A,y,m,n))\n",
    "\n",
    "uu_2=Sconvex_min2(y,A,u_0,L,mu,1000)\n",
    "#print(\"The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is\",uu_2)\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 1000 iterations is\",resi_Value(uu_2,A,y,m,n))\n",
    "\n",
    "uu_3=convex_min2(y,A,u_0,1000)\n",
    "#print(\"The minimiser from Nestorov method for strongly convex objectives after 1000 iterations is\",uu_3)\n",
    "print(\"The residual of Nestorov method for convex objectives after 1000 iterations is\",resi_Value(uu_3,A,y,m,n))\n",
    "\n",
    "##set the number of iterations as 10000\n",
    "uu_1=GDminimiser2(y,A,1/L,u_0,10000) #use a constant step size of 1/L\n",
    "#print(\"The minimiser from the method of gradient descent after 10000 iterations is\",uu_1)\n",
    "print(\"The residual of the method of gradient descent after 10000 iterations is\",resi_Value(uu_1,A,y,m,n))\n",
    "\n",
    "uu_2=Sconvex_min2(y,A,u_0,L,mu,10000)\n",
    "#print(\"The minimiser from Nestorov method for strongly convex objectives after 10000 iterations is\",uu_2)\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 10000 iterations is\",resi_Value(uu_2,A,y,m,n))\n",
    "\n",
    "uu_3=convex_min2(y,A,u_0,10000)\n",
    "#print(\"The minimiser from Nestorov method for strongly convex objectives after 10000 iterations is\",uu_3)\n",
    "print(\"The residual of Nestorov method for convex objectives after 10000 iterations is\",resi_Value(uu_3,A,y,m,n))\n",
    "\n",
    "##set the number of iterations as 100000\n",
    "uu_2=Sconvex_min2(y,A,u_0,L,mu,100000)\n",
    "#print(\"The minimiser from Nestorov method for strongly convex objectives after 100000 iterations is\",uu_2)\n",
    "print(\"The residual of Nestorov method for strongly convex objectives after 100000 iterations is\",resi_Value(uu_2,A,y,m,n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of iterations is 1,000 or 10,000, the residual(around 1.82) of gradient descents method is close to the residual(around 1.83) of Nestorov method for convex objectives. Both residuals are much smaller than the residual of Nestorov method for strongly convex objectives. In terms of residuals, both gradient descents method and Nestorov method for convex objectives perform well. In terms of the time to implement the method with the same number of iterations, the gradient method performs better. Also, when the number of iteration increases (to 10,000), the residual of Nestorov method for strongly convex objectives decreases from 49059 to 4463. However, the residual is still very large (4836 after 100,000 iterations) and it takes long time to implement Nestorov method for strongly convex objectives when the number of iterations is large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
